{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "dataset_name = \"HuggingFaceH4/MATH-500\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-19 21:39:31 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07f50338b8c14d8dabaa30d5175f298d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/679 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-19 21:40:07 config.py:542] This model supports multiple tasks: {'classify', 'generate', 'reward', 'score', 'embed'}. Defaulting to 'generate'.\n",
      "WARNING 02-19 21:40:07 arg_utils.py:1135] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 02-19 21:40:07 config.py:1556] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 02-19 21:40:07 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d11e0018df654511a0384669950c1f66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e2b68b907954363acac4f31de956cbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cfa65458e264014af49535256da2def",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 02-19 21:40:10 interface.py:284] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 02-19 21:40:13 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 02-19 21:40:13 model_runner.py:1110] Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B...\n",
      "INFO 02-19 21:40:14 weight_utils.py:252] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d2a47d51ba442a09270c315ddcc4f93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-19 21:41:40 weight_utils.py:297] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c7971f453674aae9650a008bf864d95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-19 21:41:41 model_runner.py:1115] Loading model weights took 3.3460 GB\n",
      "INFO 02-19 21:41:42 worker.py:267] Memory profiling takes 0.85 seconds\n",
      "INFO 02-19 21:41:42 worker.py:267] the current vLLM instance can use total_gpu_memory (23.99GiB) x gpu_memory_utilization (0.90) = 21.59GiB\n",
      "INFO 02-19 21:41:42 worker.py:267] model weights take 3.35GiB; non_torch_memory takes 0.08GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 16.78GiB.\n",
      "INFO 02-19 21:41:42 executor_base.py:110] # CUDA blocks: 39264, # CPU blocks: 9362\n",
      "INFO 02-19 21:41:42 executor_base.py:115] Maximum concurrency for 131072 tokens per request: 4.79x\n",
      "INFO 02-19 21:41:43 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:09<00:00,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-19 21:41:52 model_runner.py:1562] Graph capturing finished in 9 secs, took 0.22 GiB\n",
      "INFO 02-19 21:41:52 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 11.21 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-19 21:41:54 config.py:542] This model supports multiple tasks: {'classify', 'generate', 'reward', 'score', 'embed'}. Defaulting to 'generate'.\n",
      "WARNING 02-19 21:41:54 arg_utils.py:1135] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 02-19 21:41:54 config.py:1556] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 02-19 21:41:54 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=100000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 02-19 21:41:54 model_runner.py:1110] Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B...\n",
      "INFO 02-19 21:41:55 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
      "INFO 02-19 21:41:55 weight_utils.py:297] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f72ca6fbe3894c9cb6cf41a9caef4751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-19 21:41:56 model_runner.py:1115] Loading model weights took 3.3148 GB\n",
      "INFO 02-19 21:42:13 worker.py:267] Memory profiling takes 17.07 seconds\n",
      "INFO 02-19 21:42:13 worker.py:267] the current vLLM instance can use total_gpu_memory (23.99GiB) x gpu_memory_utilization (0.90) = 21.59GiB\n",
      "INFO 02-19 21:42:13 worker.py:267] model weights take 3.31GiB; non_torch_memory takes -1.95GiB; PyTorch activation peak memory takes 1.38GiB; the rest of the memory reserved for KV Cache is 18.84GiB.\n",
      "INFO 02-19 21:42:13 executor_base.py:110] # CUDA blocks: 44092, # CPU blocks: 9362\n",
      "INFO 02-19 21:42:13 executor_base.py:115] Maximum concurrency for 100000 tokens per request: 7.05x\n",
      "INFO 02-19 21:42:14 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:34<00:00,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-19 21:42:59 model_runner.py:1562] Graph capturing finished in 42 secs, took 0.00 GiB\n",
      "INFO 02-19 21:42:59 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 63.47 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed prompts: 100%|██████████| 1/1 [06:19<00:00, 379.00s/it, est. speed input: 0.16 toks/s, output: 6.02 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alright, let's tackle this problem step by step. So, the user is asking, \"How many r in raspberry?\" Hmm, first I need to figure out what \"raspberry\" refers to. Could it be a programming term, a song, or something else? Let me think.\n",
      "\n",
      "If it's a song, maybe I should check my knowledge base. But I'm more familiar with programming terms because that's where someone would ask such a question. So, \"raspberry\" could be a programming language or maybe a要注意天气或时刻有关的关键点。 Wait, but \"raspberry\" is more commonly associated with a programming concept. Alternatively, could it be a username or typo?\n",
      "\n",
      "Wait, perhaps it's \"raspberry\" as in a placeholder or a specific variable name. However, in that context, it's unclear what part of the string is being referred to. Maybe the user is referring to a specific word, like \"raspberry\" in a dictionary. Or perhaps it's a typo for something else.\n",
      "\n",
      "I should make an assumption here. If I dig deeper, \"raspberry\" is often used as a username or as part of a loop variable in programming. But without more context, it's challenging. Alternatively, it might be a reference to something else entirely. For example, in some contexts, \"rasp berry\" could be a reference to a song or an album, but I don't recall that it being the case for a specific month or song.\n",
      "\n",
      "Alternatively, maybe it's a user typo and they meant \"raspberry\" as a phone number or a username. But that seems unlikely. Alternatively, it could be a trick question where \"raspberry\" is not a programming term but just a word used at the beginning of the sentence.\n",
      "\n",
      "Given that without more context, it's difficult to say for certain. If I have to proceed, maybe I can consider two possibilities: one where \"raspberry\" is a string attribute, perhaps a variable name, and another where it's a username or typo.\n",
      "\n",
      "Another approach is to think about the word \"rasp berry.\" But that's just an expression of atIndex, so perhaps that doesn't make sense here.\n",
      "\n",
      "Wait, perhaps it's a typo for \"raspberry\" as part of a string, such as in a variable name. In Python, for instance, variables can be named any string, so if \"raspberry\" was meant to be the initial part of a string variable, it's impossible without the full context.\n",
      "\n",
      "Alternatively, if the user meant \"raspberry\" as in a value from a certain dataset or database, but again, without the data attach, that's speculative.\n",
      "\n",
      "I think, perhaps, it's better to proceed with the assumption that \"raspberry\" refers to a variable or string in the context of a Python programming problem. That is, perhaps, \"raspberry\" is part of a Python dictionary or list.\n",
      "\n",
      "Wait, but how is that? For instance, in programming challenges, sometimes a string like \"raspberry\" is used as a hint in some coding problems, such as finding the count of a character in a string or something similar.\n",
      "\n",
      "Wait, so if the user's code is trying to find how many times the character 'r' appears in the string \"raspberry\", perhaps the user is trying to parse a dictionary where the key is 'raspberry' and the value is something, but that's unclear.\n",
      "\n",
      "Alternatively, perhaps the user is looking for the first occurrence of 'r' in the word \"raspberry\".\n",
      "\n",
      "Wait, let's do that indices task. Let's take the word \"raspberry\" and index each character.\n",
      "\n",
      "1: r\n",
      "2: a\n",
      "3: s\n",
      "4: p\n",
      "5: b\n",
      "6: e\n",
      "7: r\n",
      "8: r\n",
      "9: y\n",
      "\n",
      "So, in the middle of the word \"raspberry\", it's variable \"rasp)\", well-brained? R a s p b e r r y.\n",
      "\n",
      "So, the letter 'r's are at positions 1, '5', '8', '9', but wait, 5 is e? Wait, no.\n",
      "\n",
      "Wait:\n",
      "\n",
      "Let's count each letter:\n",
      "\n",
      "1: r\n",
      "So, 'r's in \"raspberry\" are at the first character, then the fourth character (p), wait, no. Wait, let's read the word correctly.\n",
      "\n",
      "Wait, no: r-a-s-p-b-e-r-r-y. So, it's r (1), a (2), s (3), p (4), b (5), e (6), r (7), r (8), y (9). So, the 'r's are at positions 1, 7, 8, and 4? Wait, position 4 is p.\n",
      "\n",
      "Wait, hold on. Let me number each character:\n",
      "\n",
      "1: r\n",
      "2: a\n",
      "3: s\n",
      "4: p\n",
      "5: b\n",
      "6: e\n",
      "7: r\n",
      "8: r\n",
      "9: y\n",
      "\n",
      "Wait, so position 1 is 'r', position7: 'r', position8: 'r'. So, that's three 'r's.\n",
      "\n",
      "Wait, but wait, that can't be. Because the word is \"raspberry,\" so r-a-s-p-b-e-r-r-y:\n",
      "\n",
      "r a s p b e r r y\n",
      "\n",
      "Wait, that's six letters, but no:\n",
      "\n",
      "Wait, let's spell it out:\n",
      "\n",
      "First part: 'rasp' is 4 letters:\n",
      "\n",
      "r, a, s, p.\n",
      "\n",
      "Then: 'b' is fifth.\n",
      "\n",
      "Then: 'e' is sixth.\n",
      "\n",
      "Then: 'r' is seventh.\n",
      "\n",
      "Then: 'r' is eighth.\n",
      "\n",
      "Then: 'y' is ninth.\n",
      "\n",
      "So yes, 4 letters in 'rasp' (4 letters), then 'b', 'e', 'r', 'r', 'y'.\n",
      "\n",
      "Wait, no, actually, \"raspberry\" is 9 letters: R-A-S-P-B-E-R-R-Y.\n",
      "\n",
      "Wait, that's 9 letters.\n",
      "\n",
      "So positions 1: R, 2: A, 3:S,4:P,5:B,6:E,7:R,8:R,9:Y.\n",
      "\n",
      "So how many 'r's are there? Let me look.\n",
      "\n",
      "Positions 1: Yes.\n",
      "\n",
      "7: Yes.\n",
      "\n",
      "8: Yes.\n",
      "\n",
      "Wait, 9th letter is Y, not r.\n",
      "\n",
      "Wait, correction: 1: R,7: R,8: R.\n",
      "\n",
      "So three 'r's in the word \"raspberry.\" So, the count is 3.\n",
      "\n",
      "But wait, is that the correct answer for the user's own question?\n",
      "\n",
      "Because, in fact, the user's initial question is, How many r in raspberry?\n",
      "\n",
      "I think \"raspberry\" is referring to the count of 'r's in 'raspberry', which is 3.\n",
      "\n",
      "Alternatively, perhaps \"raspberry\" is being used as a key in a dictionary, but without knowing the code, that's speculative.\n",
      "\n",
      "But, given that, if someone is trying to parse a dictionary and looking up \"raspberry\" without knowing its value, like maybe in a question, such as:\n",
      "\n",
      "\"Create a dictionary with 'raspberry': 'apple', then count how many r's: How many r in raspberry?\"\n",
      "\n",
      "Then they might be trying to count the number of 'r's, but in that case, the value might be empty, and count would be zero.\n",
      "\n",
      "Alternatively, perhaps it's the opposite.\n",
      "\n",
      "But without more context, I think the best assumption is that the problem is to count 'r's in \"raspberry.\"\n",
      "\n",
      "Assuming that, the answer is three 'r's.\n",
      "\n",
      "But to make sure, let me think again.\n",
      "\n",
      "raspberry is spelled as r-a-s-p-b-e-r-r-y. So, positions 1: r(1), and positions 7: r(7), positions 8: r(8). So 3 in total.\n",
      "\n",
      "Thus, the answer is 3.\n",
      "\n",
      "Alternatively, if \"raspberry\" is part of a variable name in Python, such as a key or value, then count of 'r' in that variable would be... but without code, can't tell.\n",
      "\n",
      "But is it 3?\n",
      "\n",
      "Alternatively, perhaps it's a typo for \"raspberry\" as a string and in a coding challenge, especially bash or another shell, but given thatbash is used for command-line, but not for strings.\n",
      "\n",
      "Alternatively, it's a keyword search, but likely not.\n",
      "\n",
      "Alternatively, the problem is just called rap-BERRY and the user mistyped a ' doesn't exist.\n",
      "\n",
      "Alternatively, no; perhaps I should confirm.\n",
      "\n",
      "Wait, perhaps it's a subject matter where \"raspberry\" is a programming term, in which case, it should be straightforward.\n",
      "\n",
      "Considering that, to sum up, if \"raspberry\" were a string in code, it's likely we are to count 'r's in it, which is 3.\n",
      "\n",
      "Alternatively, perhaps it's just a made-up problem.\n",
      "\n",
      "Hm.\n",
      "\n",
      "Wait, could I have mixed it up with \"raspberry\"?\n",
      "\n",
      "Wait, \"raspberry\" is a word meaning to hurt, but also used as a shirt type in racing, but that's physical, so not relevant here.\n",
      "\n",
      "Wait, perhaps there's a question typo, so maybe, the correct word is \"raspberry,\" and the user mistyped a '!' letter, but that is for SPSS commands or file names and is unrelated.\n",
      "\n",
      "Alternatively, another approach: let me see if often, without parenthetical, \"raspberry\" is typing code.\n",
      "\n",
      "Wait, perhaps, but impo.\n",
      "\n",
      "Given all this, the safest initial answer I can make is that the word \"raspberry\" is being referred to, and the problem is to count the number of 'r's in it.\n",
      "\n",
      "So,\n",
      "\n",
      "raspberry: r, a, s, p, b, e, r, r, y.\n",
      "\n",
      "Count of 'r's: 3.\n",
      "\n",
      "But if it's a variable name, then the count could vary.\n",
      "\n",
      "But I think the intended answer is three.\n",
      "\n",
      "Therefore, the final answer is 3.\n",
      "\n",
      "**Final Answer**\n",
      "The number of 'r's in raspberry is \\boxed{3}.\n",
      "</think>\n",
      "\n",
      "The user is asking, \"How many r in raspberry?\" After considering various possibilities and interpretations, the most likely assumption is that the word \"raspberry\" refers to the count of 'r's in it.\n",
      "\n",
      "The word \"raspberry\" is spelled as: r-a-s-p-b-e-r-r-y.\n",
      "\n",
      "Counting each 'r' in the word:\n",
      "1. The first character is 'r'.\n",
      "2. The seventh character is 'r'.\n",
      "3. The eighth character is 'r'.\n",
      "4. The 10th character is 'y' (not 'r').\n",
      "\n",
      "Thus, there are three 'r's in the word \"raspberry\".\n",
      "\n",
      "### Final Answer\n",
      "The number of 'r's in raspberry is \\boxed{3}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model = LLM(model_name)\n",
    "tok = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "stop_token_ids = tok(\"<|im_end|>\")[\"input_ids\"]\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    max_tokens=8192,\n",
    "    min_tokens=0,\n",
    "    stop_token_ids=stop_token_ids,\n",
    ")\n",
    "model = LLM(model_name, max_model_len=100000)\n",
    "prompt = \"How many r in raspberry\"\n",
    "prompt = (\n",
    "    \"<|im_start|>system\\nYou are DeepSeek-R1, created by DeepSeek. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n\"\n",
    "    + prompt\n",
    "    + \"<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    ")\n",
    "\n",
    "o = model.generate(prompt, sampling_params=sampling_params)\n",
    "print(o[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
